{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 pytorch初步应用\n",
    "构建神经网络：torch.nn 依赖autograd自动求导\n",
    "\n",
    "流程：\n",
    "- 定义一个拥有可学习参数的神经网络\n",
    "- 遍历训练数据集，处理输入数据使其流经神经网络\n",
    "- 计算损失值\n",
    "- 将网络参数的梯度进行反向传播\n",
    "- 以一定的规则更新网络的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        # 网络层定义\n",
    "        # 定义第一层卷积神经网络, 输入通道维度=1, 输出通道维度=6, 卷积核大小3*3\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        # 定义第二层卷积神经网络, 输入通道维度=6, 输出通道维度=16, 卷积核大小3*3\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # 定义三层全连接网络\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 在(2, 2)的池化窗口下执行最大池化操作\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        # 计算size, 除了第0个维度上的batch_size\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 576])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n",
      "tensor([[-0.0390, -0.1032, -0.1672,  0.0213, -0.0284,  0.0307, -0.0461,  0.0201,\n",
      "         -0.0496, -0.0904]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "# print(params[0].size()) \n",
    "for p in params:\n",
    "    print(p.size())\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "'''\n",
    "torch.nn构建的神经网络只支持mini-batches的输入, 不支持单一样本的输入.\n",
    "比如: nn.Conv2d 需要一个4D Tensor, 形状为(nSamples, nChannels, Height, Width). \n",
    "如果你的输入只有单一样本形式, 则需要执行input.unsqueeze(0), 主动将3D Tensor扩充成4D Tensor.\n",
    "'''\n",
    "out = net.forward(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 计算loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([1, 10])\n",
      "tensor(0.9437, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out.size())\n",
    "target  = torch.randn(10)\n",
    "print(target.size())\n",
    "target = target.view(1,-1)\n",
    "print(target.size())\n",
    "c = nn.MSELoss()\n",
    "loss = c(out,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x7f136ce777f0>\n",
      "<AddmmBackward0 object at 0x7f136ce77ee0>\n",
      "<AccumulateGrad object at 0x7f136ce777f0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([ 0.0127, -0.0005, -0.0009,  0.0025, -0.0058,  0.0057])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2442, -0.1785, -0.0841],\n",
      "         [ 0.0188, -0.1052,  0.1658],\n",
      "         [ 0.2443,  0.3327,  0.3157]]], grad_fn=<SelectBackward0>)\n",
      "tensor([[[-0.2441, -0.1783, -0.0840],\n",
      "         [ 0.0189, -0.1050,  0.1655],\n",
      "         [ 0.2440,  0.3328,  0.3157]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 更新参数\n",
    "import torch.optim as optim\n",
    "net.zero_grad()\n",
    "print(list(net.parameters())[0][0])\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = c(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(list(net.parameters())[0][0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98a99fa221f06f708072c4ee4ae796a4fc4d1412fb20fefb4ecbaa74a53967e3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py385': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
