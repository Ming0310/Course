{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION\n",
    "什么是注意力:\n",
    "- 我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.\n",
    "\n",
    "什么是注意力计算规则:\n",
    "- 它需要三个指定的输入Q(query), K(key), V(value), 然后通过计算公式得到注意力的结果, 这个结果代表query在key和value作用下的注意力表示. 当输入的Q=K=V时, 称作自注意力计算规则.\n",
    "\n",
    "常见的注意力计算规则:\n",
    "<img src='attention.png'>\n",
    "\n",
    "什么是注意力机制:\n",
    "- 注意力机制是注意力计算规则能够应用的深度学习网络的载体, 同时包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制.\n",
    "\n",
    "注意力机制的作用:\n",
    "\n",
    "当前的注意力机制大多数应用于seq2seq架构, 即编码器和解码器模型.\n",
    "- 在解码器端的注意力机制: 能够根据模型目标有效的聚焦编码器的输出结果, 当其作为解码器的输入时提升效果. 改善以往编码器输出是单一定长张量, 无法存储过多信息的情况.\n",
    "- 在编码器端的注意力机制: 主要解决表征问题, 相当于特征提取过程, 得到输入的注意力表示. 一般使用自注意力(self-attention).\n",
    "\n",
    "参考：\n",
    "- https://zhuanlan.zhihu.com/p/43493999\n",
    "- https://blog.csdn.net/malefactor/article/details/78767781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 1, 64])\n",
      "tensor([[[-0.4893, -0.0382,  0.0750,  0.7484,  0.5957,  0.4762, -0.1784,\n",
      "          -0.0109, -0.2045,  0.1959, -0.5979, -0.0367,  0.1800, -0.1001,\n",
      "          -0.3183,  0.0361,  0.1113, -0.4263,  0.1688,  0.0389,  0.0921,\n",
      "          -0.0892, -0.2930,  0.3485,  0.4557, -0.1532,  0.0632, -0.3931,\n",
      "          -0.0258, -0.5209,  0.4012, -0.0616, -0.5532,  0.5394,  0.3082,\n",
      "           0.0811,  0.5599,  0.0127, -0.3955,  0.1107, -0.3800,  0.2598,\n",
      "           0.0978,  0.5679, -0.3533, -0.3098, -0.0513, -0.2024,  0.3421,\n",
      "          -0.9749, -0.1686, -0.2651, -0.0335,  0.1254, -0.1123, -0.2561,\n",
      "           0.0803, -0.5917,  0.1698,  0.2969, -0.4227, -0.0950, -0.0270,\n",
      "           0.2790]]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[[-0.4254, -0.1460, -0.1529,  0.3730,  0.3072, -0.1046, -0.0215,\n",
      "          -0.1207, -0.1985,  0.0771,  0.0965,  0.0494,  0.0455,  0.3402,\n",
      "          -0.4649, -0.2979, -0.1114,  0.1001, -0.1541, -0.2596, -0.0677,\n",
      "          -0.1553, -0.5134,  0.0261,  0.2263, -0.2201,  0.1677,  0.0261,\n",
      "          -0.2132,  0.1887,  0.1310,  0.1105,  0.1520,  0.0257, -0.0129,\n",
      "           0.1188, -0.3089, -0.1566, -0.2191,  0.1257,  0.3840,  0.3511,\n",
      "           0.1263, -0.0906,  0.2177, -0.4217,  0.3969,  0.1383,  0.0721,\n",
      "           0.3223, -0.0863, -0.1676,  0.2374, -0.5082, -0.2088, -0.3151,\n",
      "           0.0807, -0.0673, -0.1118,  0.2490, -0.0513,  0.2676, -0.0441,\n",
      "           0.0434]]], grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size1, value_size2, output_size):\n",
    "        \"\"\"初始化函数中的参数有5个, query_size代表query的最后一维大小\n",
    "           key_size代表key的最后一维大小, value_size1代表value的倒数第二维大小,\n",
    "           value = (1, value_size1, value_size2)\n",
    "           value_size2代表value的倒数第一维大小, output_size输出的最后一维大小\"\"\"\n",
    "        super(Attn, self).__init__()\n",
    "        # 将以下参数传入类中\n",
    "        self.query_size = query_size\n",
    "        self.key_size = key_size\n",
    "        self.value_size1 = value_size1\n",
    "        self.value_size2 = value_size2\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # 初始化注意力机制实现第一步中需要的线性层.\n",
    "        self.attn = nn.Linear(self.query_size + self.key_size, value_size1)\n",
    "\n",
    "        # 初始化注意力机制实现第三步中需要的线性层.\n",
    "        self.attn_combine = nn.Linear(self.query_size + value_size2, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        \"\"\"forward函数的输入参数有三个, 分别是Q, K, V, 根据模型训练常识, 输入给Attion机制的\n",
    "           张量一般情况都是三维张量, 因此这里也假设Q, K, V都是三维张量\"\"\"\n",
    "\n",
    "        # 第一步, 按照计算规则进行计算,\n",
    "        # 我们采用常见的第一种计算规则\n",
    "        # 将Q，K进行纵轴拼接, 做一次线性变化, 最后使用softmax处理获得结果(注意力向量)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((Q[0], K[0]), 1)), dim=1)\n",
    "\n",
    "        # 然后进行第一步的后半部分, 将得到的权重矩阵与V做矩阵乘法计算,\n",
    "        # 当二者都是三维张量且第一维代表为batch条数时, 则做bmm运算\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), V)\n",
    "\n",
    "        # 之后进行第二步, 通过取[0]是用来降维, 根据第一步采用的计算方法,\n",
    "        # 需要将Q与第一步的计算结果再进行拼接\n",
    "        output = torch.cat((Q[0], attn_applied[0]), 1)\n",
    "\n",
    "        # 最后是第三步, 使用线性层作用在第三步的结果上做一个线性变换并扩展维度，得到输出\n",
    "        # 因为要保证输出也是三维张量, 因此使用unsqueeze(0)扩展维度\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        return output, attn_weights, attn_applied\n",
    "\n",
    "\n",
    "query_size = 32\n",
    "key_size = 32\n",
    "value_size1 = 32\n",
    "value_size2 = 64\n",
    "output_size = 64\n",
    "attn = Attn(query_size, key_size, value_size1, value_size2, output_size)\n",
    "Q = torch.randn(1,1,32) \n",
    "K = torch.randn(1,1,32)\n",
    "V = torch.randn(1,32,64)\n",
    "# Q = torch.randn(1,1,64)\n",
    "# K = torch.randn(1,1,64) #错误如何改进\n",
    "# V = torch.randn(1,32,64)\n",
    "out = attn(Q, K ,V)\n",
    "print(out[0].shape)\n",
    "print(out[1].shape)\n",
    "print(out[2].shape)\n",
    "print(out[0]) #这个结果代表query在key和value作用下的注意力表示\n",
    "# print(out[1]) #相似度\n",
    "print(out[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子。比如在预测“我妈今天做的这顿饭真好吃”的情感时，如果只预测正向还是负向，那真正影响结果的只有“真好吃”这三个字，前面说的“我妈今天做的这顿饭”基本没什么用，如果是直接对token embedding进行平均去求句子表示会引入不少噪声。所以引入attention机制，让我们可以根据任务目标赋予输入token不同的权重，理想情况下前半句的权重都在0.0及，后三个字则是“0.3, 0.3, 0.3”，在计算句子表示时就变成了：\n",
    "\n",
    "最终表示 = 0.01x我+0.01x妈+0.01x今+0.01x天+0.01x做+0.01x的+0.01x这+0.01x顿+0.02x饭+0.3x真+0.3x好+0.3x吃\n",
    "\n",
    "机器翻译：\n",
    "<!-- torch.Size([25])\n",
    "torch.Size([1, 1, 25])\n",
    "经过线性映射\n",
    "torch.Size([10, 25]) -->\n",
    "Q = torch.randn(1,1,32) 待翻译语言的一个词表示 batch_size,一个词, 编码器hidden_size\n",
    "\n",
    "K = torch.randn(1,1,32) 编码器输出的整句表示\n",
    "\n",
    "V = torch.randn(1,32,64) 编码器输出的所有词表示 batch_size, 句子序列长度（词数），hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果参数1形状是(b × n × m), 参数2形状是(b × m × p), 则输出为(b × n × p) 矩阵乘法\n",
    "input = torch.randn(10, 3, 4)\n",
    "mat2 = torch.randn(10, 4, 5)\n",
    "res = torch.bmm(input, mat2)\n",
    "res.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用seq2seq模型架构实现英译法任务\n",
    "学习目标:\n",
    "- 更深一步了解seq2seq模型架构和翻译数据集.\n",
    "- 掌握使用基于GRU的seq2seq模型架构实现翻译的过程.\n",
    "- 掌握Attention机制在解码器端的实现过程.\n",
    "\n",
    "<img src='seq2seq.jpg'>\n",
    "\n",
    "从图中可知, seq2seq模型架构, 包括两部分分别是encoder(编码器)和decoder(解码器), 编码器和解码器的内部实现都使用了GRU模型, 这里它要完成的是一个中文到英文的翻译: 欢迎 来 北京 --> welcome to BeiJing. 编码器首先处理中文输入\"欢迎 来 北京\", 通过GRU模型获得每个时间步的输出张量，最后将它们拼接成一个中间语义张量c, 接着解码器将使用这个中间语义张量c以及每一个时间步的隐层张量, 逐个生成对应的翻译语言."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yangcaihua/task/base_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Go.\\tVa !\\n',\n",
       " 'Run!\\tCours\\u202f!\\n',\n",
       " 'Run!\\tCourez\\u202f!\\n',\n",
       " 'Wow!\\tÇa alors\\u202f!\\n',\n",
       " 'Fire!\\tAu feu !\\n',\n",
       " \"Help!\\tÀ l'aide\\u202f!\\n\",\n",
       " 'Jump.\\tSaute.\\n',\n",
       " 'Stop!\\tÇa suffit\\u202f!\\n',\n",
       " 'Stop!\\tStop\\u202f!\\n',\n",
       " 'Stop!\\tArrête-toi !\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "f = open('./data/data/eng-fra.txt','r')\n",
    "data = f.readlines()\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于GRU的seq2seq模型架构实现翻译的过程:\n",
    "\n",
    "- 第一步: 导入必备的工具包.\n",
    "- 第二步: 对持久化文件中数据进行处理, 以满足模型训练要求.\n",
    "- 第三步: 构建基于GRU的编码器和解码器.\n",
    "- 第四步: 构建模型训练函数, 并进行训练.\n",
    "- 第五步: 构建模型评估函数, 并进行测试以及Attention效果分析."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从io工具包导入open方法\n",
    "from io import open\n",
    "# 用于字符规范化\n",
    "import unicodedata\n",
    "# 用于正则表达式\n",
    "import re\n",
    "# 用于随机生成数据\n",
    "import random\n",
    "# 用于构建网络结构和函数的torch工具包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# torch中预定义的优化方法工具包\n",
    "from torch import optim\n",
    "# 设备选择, 我们可以选择在cuda或者cpu上运行你的代码\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 起始标志\n",
    "SOS_token = 0\n",
    "# 结束标志\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        \"\"\"初始化函数中参数name代表传入某种语言的名字\"\"\"\n",
    "        # 将name传入类中\n",
    "        self.name = name\n",
    "        # 初始化词汇对应自然数值的字典\n",
    "        self.word2index = {}\n",
    "        # 初始化自然数值对应词汇的字典, 其中0，1对应的SOS和EOS已经在里面了\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        # 初始化词汇对应的自然数索引，这里从2开始，因为0，1已经被开始和结束标志占用了\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"添加句子函数, 即将句子转化为对应的数值序列, 输入参数sentence是一条句子\"\"\"\n",
    "        # 根据一般国家的语言特性(我们这里研究的语言都是以空格分个单词)\n",
    "        # 对句子进行分割，得到对应的词汇列表\n",
    "        for word in sentence.split(' '):\n",
    "            # 然后调用addWord进行处理\n",
    "            self.addWord(word)\n",
    "\n",
    "\n",
    "    def addWord(self, word):\n",
    "        \"\"\"添加词汇函数, 即将词汇转化为对应的数值, 输入参数word是一个单词\"\"\"\n",
    "        # 首先判断word是否已经在self.word2index字典的key中\n",
    "        if word not in self.word2index:\n",
    "            # 如果不在, 则将这个词加入其中, 并为它对应一个数值，即self.n_words\n",
    "            self.word2index[word] = self.n_words\n",
    "            # 同时也将它的反转形式加入到self.index2word中\n",
    "            self.index2word[self.n_words] = word\n",
    "            # self.n_words一旦被占用之后，逐次加1, 变成新的self.n_words\n",
    "            self.n_words += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2index: {'hello': 2, 'I': 3, 'am': 4, 'Jay': 5}\n",
      "index2word: {0: 'SOS', 1: 'EOS', 2: 'hello', 3: 'I', 4: 'am', 5: 'Jay'}\n",
      "n_words: 6\n"
     ]
    }
   ],
   "source": [
    "name = \"eng\"\n",
    "sentence = \"hello I am Jay\"\n",
    "engl = Lang(name)\n",
    "engl.addSentence(sentence)\n",
    "print(\"word2index:\", engl.word2index)\n",
    "print(\"index2word:\", engl.index2word)\n",
    "print(\"n_words:\", engl.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将unicode转为Ascii, 我们可以认为是去掉一些语言中的重音标记：Ślusàrski\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"字符串规范化函数, 参数s代表传入的字符串\"\"\"\n",
    "    # 使字符变为小写并去除两侧空白符, z再使用unicodeToAscii去掉重音标记\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # 在.!?前加一个空格\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # 使用正则表达式将字符串中不是大小写字母和正常标点的都替换成空格\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you kidding me ?\n"
     ]
    }
   ],
   "source": [
    "s = \"Are you kidding me?\"\n",
    "nsr = normalizeString(s)\n",
    "print(nsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/data/eng-fra.txt'\n",
    "\n",
    "def readLangs(lang1, lang2):\n",
    "    \"\"\"读取语言函数, 参数lang1是源语言的名字, 参数lang2是目标语言的名字\n",
    "       返回对应的class Lang对象, 以及语言对列表\"\"\"\n",
    "    # 从文件中读取语言对并以/n划分存到列表lines中\n",
    "    lines = open(data_path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # 对lines列表中的句子进行标准化处理，并以\\t进行再次划分, 形成子列表, 也就是语言对\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] \n",
    "    # 然后分别将语言名字传入Lang类中, 获得对应的语言对象, 返回结果\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_lang: <__main__.Lang object at 0x7f83d4d81130>\n",
      "output_lang: <__main__.Lang object at 0x7f83656c8130>\n",
      "pairs中的前五个: [['go .', 'va !'], ['run !', 'cours !'], ['run !', 'courez !'], ['wow !', 'ca alors !'], ['fire !', 'au feu !']]\n"
     ]
    }
   ],
   "source": [
    "lang1 = \"eng\"\n",
    "lang2 = \"fra\"\n",
    "input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "print(\"input_lang:\", input_lang)\n",
    "print(\"output_lang:\", output_lang)\n",
    "print(\"pairs中的前五个:\", pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置组成句子中单词或标点的最多个数\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# 选择带有指定前缀的语言特征数据作为训练数据\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    \"\"\"语言对过滤函数, 参数p代表输入的语言对, 如['she is afraid.', 'elle malade.']\"\"\"\n",
    "    # p[0]代表英语句子，对它进行划分，它的长度应小于最大长度MAX_LENGTH并且要以指定的前缀开头\n",
    "    # p[1]代表法文句子, 对它进行划分，它的长度应小于最大长度MAX_LENGTH\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes) and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    \"\"\"对多个语言对列表进行过滤, 参数pairs代表语言对组成的列表, 简称语言对列表\"\"\"\n",
    "    # 函数中直接遍历列表中的每个语言对并调用filterPair即可\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后的pairs前五个: [['i m .', 'j ai ans .'], ['i m ok .', 'je vais bien .'], ['i m ok .', 'ca va .'], ['i m fat .', 'je suis gras .'], ['i m fat .', 'je suis gros .']]\n"
     ]
    }
   ],
   "source": [
    "# 输入参数pairs使用readLangs函数的输出结果pairs\n",
    "fpairs = filterPairs(pairs)\n",
    "print(\"过滤后的pairs前五个:\", fpairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2):\n",
    "    \"\"\"数据准备函数, 完成将所有字符串数据向数值型数据的映射以及过滤语言对\n",
    "       参数lang1, lang2分别代表源语言和目标语言的名字\"\"\"\n",
    "    # 首先通过readLangs函数获得input_lang, output_lang对象，以及字符串类型的语言对列表\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "    # 对字符串类型的语言对列表进行过滤操作\n",
    "    pairs = filterPairs(pairs)\n",
    "    # 对过滤后的语言对列表进行遍历\n",
    "    for pair in pairs:\n",
    "        # 并使用input_lang和output_lang的addSentence方法对其进行数值映射\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    # 返回数值映射后的对象, 和过滤后语言对\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_n_words: 2803\n",
      "output_n_words: 4345\n",
      "['you re taller than i am .', 'tu es plus grande que moi .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'fra')\n",
    "print(\"input_n_words:\", input_lang.n_words)\n",
    "print(\"output_n_words:\", output_lang.n_words)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(lang, sentence):\n",
    "    \"\"\"将文本句子转换为张量, 参数lang代表传入的Lang的实例化对象, sentence是预转换的句子\"\"\"\n",
    "    # 对句子进行分割并遍历每一个词汇, 然后使用lang的word2index方法找到它对应的索引\n",
    "    # 这样就得到了该句子对应的数值列表\n",
    "    indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    # 然后加入句子结束标志\n",
    "    indexes.append(EOS_token)\n",
    "    # 将其使用torch.tensor封装成张量, 并改变它的形状为nx1, 以方便后续计算\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    \"\"\"将语言对转换为张量对, 参数pair为一个语言对\"\"\"\n",
    "    # 调用tensorFromSentence分别将源语言和目标语言分别处理，获得对应的张量表示\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    # 最后返回它们组成的元组\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i m .', 'j ai ans .']\n",
      "(tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [1]], device='cuda:0'), tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# 取pairs的第一条\n",
    "pair = pairs[0]\n",
    "print(pair)\n",
    "pair_tensor = tensorsFromPair(pair)\n",
    "print(pair_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "tensor([2], device='cuda:0')\n",
      "torch.Size([1, 1, 25])\n",
      "torch.Size([1, 1, 25])\n",
      "tensor([-0.0254,  0.0228, -0.1346,  0.3894,  0.1858, -0.1637, -0.0146,  0.0163,\n",
      "        -0.4015, -0.0347,  0.1751, -0.0388, -0.3266,  0.1680, -0.1460,  0.1826,\n",
      "         0.2827,  0.1239, -0.2258, -0.1448,  0.2776,  0.1969, -0.1018, -0.1085,\n",
      "         0.4144], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 1, 25])\n",
      "tensor([[[-0.0254,  0.0228, -0.1346,  0.3894,  0.1858, -0.1637, -0.0146,\n",
      "           0.0163, -0.4015, -0.0347,  0.1751, -0.0388, -0.3266,  0.1680,\n",
      "          -0.1460,  0.1826,  0.2827,  0.1239, -0.2258, -0.1448,  0.2776,\n",
      "           0.1969, -0.1018, -0.1085,  0.4144]]], device='cuda:0',\n",
      "       grad_fn=<CudnnRnnBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 构建基于GRU的编码器和解码器\n",
    "# 熟悉GRU的使用\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"它的初始化参数有两个, input_size代表解码器的输入尺寸即源语言的\n",
    "            词表大小，hidden_size代表GRU的隐层节点数, 也代表词嵌入维度, 同时又是GRU的输入尺寸\"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # 将参数hidden_size传入类中\n",
    "        self.hidden_size = hidden_size\n",
    "        # 实例化nn中预定义的Embedding层, 它的参数分别是input_size, hidden_size\n",
    "        # 这里的词嵌入维度即hidden_size\n",
    "        # nn.Embedding的演示在该代码下方\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # 然后实例化nn中预定义的GRU层, 它的参数是hidden_size\n",
    "        # nn.GRU的演示在该代码下方\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"编码器前向逻辑函数中参数有两个, input代表源语言的Embedding层输入张量\n",
    "           hidden代表编码器层gru的初始隐层张量\"\"\"\n",
    "        # 将输入张量进行embedding操作, 并使其形状变为(1,1,-1),-1代表自动计算维度\n",
    "        # 理论上，我们的编码器每次只以一个词作为输入, 因此词汇映射后的尺寸应该是[1, embedding]\n",
    "        # 而这里转换成三维的原因是因为torch中预定义gru必须使用三维张量作为输入, 因此我们拓展了一个维度\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # 然后将embedding层的输出和传入的初始hidden作为gru的输入传入其中,\n",
    "        # 获得最终gru的输出output和对应的隐层张量hidden， 并返回结果\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"初始化隐层张量函数\"\"\"\n",
    "        # 将隐层张量初始化成为1x1xself.hidden_size大小的0张量\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "hidden_size = 25\n",
    "input_size = 20\n",
    "# pair_tensor[0]代表源语言即英文的句子，pair_tensor[0][0]代表句子中的第一个词\n",
    "input = pair_tensor[0][0]\n",
    "# 初始化第一个隐层张量，1x1xhidden_size的0张量\n",
    "hidden = torch.zeros(1, 1, hidden_size,device=device)\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size).to(device)\n",
    "print(input.shape)\n",
    "print(input)\n",
    "print(hidden.shape)\n",
    "encoder_output, hidden = encoder(input, hidden)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output[0,0])\n",
    "print(hidden.shape)\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建基于GRU和Attention的解码器\n",
    "# 用于探索attention的使用\n",
    "# attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        \"\"\"初始化函数中的参数有4个, hidden_size代表解码器中GRU的输入尺寸，也是它的隐层节点数\n",
    "           output_size代表整个解码器的输出尺寸, 也是我们希望得到的指定尺寸即目标语言的词表大小\n",
    "           dropout_p代表我们使用dropout层时的置零比率，默认0.1, max_length代表句子的最大长度\"\"\"\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # 将以下参数传入类中\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # 实例化一个Embedding层, 输入参数是self.output_size和self.hidden_size\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # 根据attention的QKV理论，attention的输入参数为三个Q，K，V，\n",
    "        # 第一步，使用Q与K进行attention权值计算得到权重矩阵, 再与V做矩阵乘法, 得到V的注意力表示结果.\n",
    "        # 这里常见的计算方式有三种:\n",
    "        # 1，将Q，K进行纵轴拼接, 做一次线性变化, 再使用softmax处理获得结果最后与V做张量乘法\n",
    "        # 2，将Q，K进行纵轴拼接, 做一次线性变化后再使用tanh函数激活, 然后再进行内部求和, 最后使用softmax处理获得结果再与V做张量乘法\n",
    "        # 3，将Q与K的转置做点积运算, 然后除以一个缩放系数, 再使用softmax处理获得结果最后与V做张量乘法\n",
    "\n",
    "        # 说明：当注意力权重矩阵和V都是三维张量且第一维代表为batch条数时, 则做bmm运算.\n",
    "\n",
    "        # 第二步, 根据第一步采用的计算方法, 如果是拼接方法，则需要将Q与第二步的计算结果再进行拼接, \n",
    "        # 如果是转置点积, 一般是自注意力, Q与V相同, 则不需要进行与Q的拼接.因此第二步的计算方式与第一步采用的全值计算方法有关.\n",
    "        # 第三步，最后为了使整个attention结构按照指定尺寸输出, 使用线性层作用在第二步的结果上做一个线性变换. 得到最终对Q的注意力表示.\n",
    "\n",
    "        # 我们这里使用的是第一步中的第一种计算方式, 因此需要一个线性变换的矩阵, 实例化nn.Linear\n",
    "        # 因为它的输入是Q，K的拼接, 所以输入的第一个参数是self.hidden_size * 2，第二个参数是self.max_length\n",
    "        # 这里的Q是解码器的Embedding层的输出, K是解码器GRU的隐层输出，因为首次隐层还没有任何输出，会使用编码器的隐层输出\n",
    "        # 而这里的V是编码器层的输出\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        # 接着我们实例化另外一个线性层, 它是attention理论中的第四步的线性层，用于规范输出尺寸\n",
    "        # 这里它的输入来自第三步的结果, 因为第三步的结果是将Q与第二步的结果进行拼接, 因此输入维度是self.hidden_size * 2\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        # 接着实例化一个nn.Dropout层，并传入self.dropout_p\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        # 之后实例化nn.GRU, 它的输入和隐层尺寸都是self.hidden_size\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        # 最后实例化gru后面的线性层，也就是我们的解码器输出层.\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    # 将decoder_input(法语), decoder_hidden（encoder隐层输出）, encoder_outputs（每个位置输出）即attention中的QKV\n",
    "    # QK 为啥代表相似度\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"forward函数的输入参数有三个, 分别是源数据输入张量, 初始的隐层张量, 以及解码器的输出张量\"\"\"\n",
    "\n",
    "        # 根据结构计算图, 输入张量进行Embedding层并扩展维度\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # 使用dropout进行随机丢弃，防止过拟合\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # 进行attention的权重计算, 我们呢使用第一种计算方式：\n",
    "        # 将Q，K进行纵轴拼接, 做一次线性变化, 最后使用softmax处理获得结果\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "\n",
    "        # 然后进行第一步的后半部分, 将得到的权重矩阵与V做矩阵乘法计算, 当二者都是三维张量且第一维代表为batch条数时, 则做bmm运算\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # 之后进行第二步, 通过取[0]是用来降维, 根据第一步采用的计算方法, 需要将Q与第一步的计算结果再进行拼接\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "\n",
    "        # 最后是第三步, 使用线性层作用在第三步的结果上做一个线性变换并扩展维度，得到输出\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        # attention结构的结果使用relu激活\n",
    "        output = F.relu(output)\n",
    "\n",
    "        # 将激活后的结果作为gru的输入和hidden一起传入其中\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # 最后将结果降维并使用softmax处理得到最终的结果\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        # 返回解码器结果，最后的隐层张量以及注意力权重张量\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"初始化隐层张量函数\"\"\"\n",
    "        # 将隐层张量初始化成为1x1xself.hidden_size大小的0张量\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1, 1, 25])\n",
      "torch.Size([10, 25])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 1, 25])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 25\n",
    "output_size = 10\n",
    "input = pair_tensor[1][0]\n",
    "\n",
    "hidden = torch.zeros(1, 1, hidden_size,device=device)\n",
    "# encoder_outputs需要是encoder中每一个时间步的输出堆叠而成\n",
    "# 它的形状应该是10x25, 我们这里直接随机初始化一个张量\n",
    "encoder_outputs  = torch.randn(10, 25, device=device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_size).to(device)\n",
    "print(input.shape)\n",
    "print(hidden.shape)\n",
    "print(encoder_outputs.shape)\n",
    "output, hidden, attn_weights= decoder(input, hidden, encoder_outputs)\n",
    "print(output.shape)\n",
    "print(hidden.shape)\n",
    "print(attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是teacher_forcing?\n",
    "- 它是一种用于序列生成任务的训练技巧, 在seq2seq架构中, 根据循环神经网络理论，解码器每次应该使用上一步的结果作为输入的一部分, 但是训练过程中，一旦上一步的结果是错误的，就会导致这种错误被累积，无法达到训练效果, 因此，我们需要一种机制改变上一步出错的情况，因为训练时我们是已知正确的输出应该是什么，因此可以强制将上一步结果设置成正确的输出, 这种方式就叫做teacher_forcing.\n",
    "\n",
    "teacher_forcing的作用:\n",
    "- 能够在训练的时候矫正模型的预测，避免在序列生成的过程中误差进一步放大.\n",
    "- teacher_forcing能够极大的加快模型的收敛速度，令模型训练过程更快更平稳."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建训练函数:\n",
    "# 设置teacher_forcing比率为0.5\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \"\"\"训练函数, 输入参数有8个, 分别代表input_tensor：源语言输入张量，target_tensor：目标语言输入张量，encoder, decoder：编码器和解码器实例化对象\n",
    "       encoder_optimizer, decoder_optimizer：编码器和解码器优化方法，criterion：损失函数计算方法，max_length：句子的最大长度\"\"\"\n",
    "    # loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # 初始化隐层张量\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # 编码器和解码器优化器梯度归0\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # 根据源文本和目标文本张量获得对应的长度\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # 初始化编码器输出张量，形状是max_length x encoder.hidden_size的0张量\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    # 初始设置损失为0\n",
    "    loss = 0\n",
    "\n",
    "    # 循环遍历输入张量索引\n",
    "    for ei in range(input_length):\n",
    "        # 根据索引从input_tensor取出对应的单词的张量表示，和初始化隐层张量一同传入encoder对象中\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        # 将每次获得的输出encoder_output(三维张量), 使用[0, 0]降两维变成向量依次存入到encoder_outputs\n",
    "        # 这样encoder_outputs每一行存的都是对应的句子中每个单词通过编码器的输出结果\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # 初始化解码器的第一个输入，即起始符\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # 初始化解码器的隐层张量即编码器的隐层输出\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # 根据随机数与teacher_forcing_ratio对比判断是否使用teacher_forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # 如果使用teacher_forcing\n",
    "    if use_teacher_forcing:\n",
    "        # 循环遍历目标张量索引\n",
    "        for di in range(target_length):\n",
    "            # 将decoder_input, decoder_hidden, encoder_outputs即attention中的QKV, \n",
    "            # 传入解码器对象, 获得decoder_output, decoder_hidden, decoder_attention\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # 因为使用了teacher_forcing, 无论解码器输出的decoder_output是什么, 我们都只\n",
    "            # 使用‘正确的答案’，即target_tensor[di]来计算损失\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # 并强制将下一次的解码器输入设置为‘正确的答案’\n",
    "            decoder_input = target_tensor[di]\n",
    "\n",
    "    else:\n",
    "        # 如果不使用teacher_forcing\n",
    "        # 仍然遍历目标张量索引\n",
    "        for di in range(target_length):\n",
    "            # 将decoder_input, decoder_hidden, encoder_outputs传入解码器对象\n",
    "            # 获得decoder_output, decoder_hidden, decoder_attention\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # 只不过这里我们将从decoder_output取出答案\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            # 损失计算仍然使用decoder_output和target_tensor[di]\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # 最后如果输出值是终止符，则循环停止\n",
    "            if topi.squeeze().item() == EOS_token:\n",
    "                break\n",
    "            # 否则，并对topi降维并分离赋值给decoder_input以便进行下次运算\n",
    "            # 这里的detach的分离作用使得这个decoder_input与模型构建的张量图无关，相当于全新的外界输入\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "\n",
    "    # 误差进行反向传播\n",
    "    loss.backward()\n",
    "    # 编码器和解码器进行优化即参数更新\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # 最后返回平均损失\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建时间计算函数:\n",
    "# 导入时间和数学工具包\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    \"获得每次打印的训练耗时, since是训练开始时间\"\n",
    "    # 获得当前时间\n",
    "    now = time.time()\n",
    "    # 获得时间差，就是训练耗时\n",
    "    s = now - since\n",
    "    # 将秒转化为分钟, 并取整\n",
    "    m = math.floor(s / 60)\n",
    "    # 计算剩下不够凑成1分钟的秒数\n",
    "    s -= m * 60\n",
    "    # 返回指定格式的耗时\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m 0s\n"
     ]
    }
   ],
   "source": [
    "# 假定模型训练开始时间是10min之前\n",
    "since = time.time() - 10*60\n",
    "period = timeSince(since)\n",
    "print(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入plt以便绘制损失曲线\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    \"\"\"训练迭代函数, 输入参数有6个，分别是encoder, decoder: 编码器和解码器对象，\n",
    "       n_iters: 总迭代步数, print_every:打印日志间隔, plot_every:绘制损失曲线间隔, learning_rate学习率\"\"\"\n",
    "    # 获得训练开始时间戳\n",
    "    start = time.time()\n",
    "    # 每个损失间隔的平均损失保存列表，用于绘制损失曲线\n",
    "    plot_losses = []\n",
    "\n",
    "    # 每个打印日志间隔的总损失，初始为0\n",
    "    print_loss_total = 0  \n",
    "    # 每个绘制损失间隔的总损失，初始为0\n",
    "    plot_loss_total = 0  \n",
    "\n",
    "    # 使用预定义的SGD作为优化器，将参数和学习率传入其中\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 选择损失函数\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # 根据设置迭代步进行循环\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        # 每次从语言对列表中随机取出一条作为训练语句\n",
    "        training_pair = tensorsFromPair(random.choice(pairs))\n",
    "        # 分别从training_pair中取出输入张量和目标张量\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        # 通过train函数获得模型运行的损失\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        # 将损失进行累和\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        # 当迭代步达到日志打印间隔时\n",
    "        if iter % print_every == 0:\n",
    "            # 通过总损失除以间隔得到平均损失\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            # 将总损失归0\n",
    "            print_loss_total = 0\n",
    "            # 打印日志，日志内容分别是：训练耗时，当前迭代步，当前进度百分比，当前平均损失\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        # 当迭代步达到损失绘制间隔时\n",
    "        if iter % plot_every == 0:\n",
    "            # 通过总损失除以间隔得到平均损失\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            # 将平均损失装进plot_losses列表\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            # 总损失归0\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    # 绘制损失曲线\n",
    "    plt.figure()  \n",
    "    plt.plot(plot_losses)\n",
    "    # 保存到指定路径\n",
    "    plt.savefig(\"./s2s_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置隐层大小为256 ，也是词嵌入维度      \n",
    "hidden_size = 256\n",
    "# 通过input_lang.n_words获取输入词汇总数，与hidden_size一同传入EncoderRNN类中\n",
    "# 得到编码器对象encoder1\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "\n",
    "# 通过output_lang.n_words获取目标词汇总数，与hidden_size和dropout_p一同传入AttnDecoderRNN类中\n",
    "# 得到解码器对象attn_decoder1\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "# 设置迭代步数 \n",
    "n_iters = 10000\n",
    "# 设置日志打印间隔\n",
    "print_every = 5000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 2s (5000 50%) 3.4391\n",
      "2m 6s (10000 100%) 2.7804\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 调用trainIters进行模型训练，将编码器对象encoder1，解码器对象attn_decoder1，迭代步数，日志打印间隔传入其中\n",
    "trainIters(encoder1, attn_decoder1, n_iters, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型评估函数, 并进行测试以及Attention效果分析.\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"评估函数，输入参数有4个，分别是encoder, decoder: 编码器和解码器对象，\n",
    "       sentence:需要评估的句子，max_length:句子的最大长度\"\"\"\n",
    "\n",
    "    # 评估阶段不进行梯度计算\n",
    "    with torch.no_grad():\n",
    "        # 对输入的句子进行张量表示\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        # 获得输入的句子长度\n",
    "        input_length = input_tensor.size()[0]\n",
    "        # 初始化编码器隐层张量\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # 初始化编码器输出张量，是max_lengthxencoder.hidden_size的0张量\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        # 循环遍历输入张量索引\n",
    "        for ei in range(input_length):\n",
    "             # 根据索引从input_tensor取出对应的单词的张量表示，和初始化隐层张量一同传入encoder对象中\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            #将每次获得的输出encoder_output(三维张量), 使用[0, 0]降两维变成向量依次存入到encoder_outputs\n",
    "            # 这样encoder_outputs每一行存的都是对应的句子中每个单词通过编码器的输出结果\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        # 初始化解码器的第一个输入，即起始符\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device) \n",
    "        # 初始化解码器的隐层张量即编码器的隐层输出\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # 初始化预测的词汇列表\n",
    "        decoded_words = []\n",
    "        # 初始化attention张量\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        # 开始循环解码\n",
    "        for di in range(max_length):\n",
    "            # 将decoder_input, decoder_hidden, encoder_outputs传入解码器对象\n",
    "            # 获得decoder_output, decoder_hidden, decoder_attention\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            # 取所有的attention结果存入初始化的attention张量中\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            # 从解码器输出中获得概率最高的值及其索引对象\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            # 从索引对象中取出它的值与结束标志值作对比\n",
    "            if topi.item() == EOS_token:\n",
    "                # 如果是结束标志值，则将结束标志装进decoded_words列表，代表翻译结束\n",
    "                decoded_words.append('<EOS>')\n",
    "                # 循环退出\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # 否则，根据索引找到它在输出语言的index2word字典中对应的单词装进decoded_words\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            # 最后将本次预测的索引降维并分离赋值给decoder_input，以便下次进行预测\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        # 返回结果decoded_words， 以及完整注意力张量, 把没有用到的部分切掉\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选择指定数量的数据进行评估:\n",
    "def evaluateRandomly(encoder, decoder, n=6):\n",
    "    \"\"\"随机测试函数, 输入参数encoder, decoder代表编码器和解码器对象，n代表测试数\"\"\"\n",
    "    # 对测试数进行循环\n",
    "    for i in range(n):\n",
    "        # 从pairs随机选择语言对\n",
    "        pair = random.choice(pairs)\n",
    "        # > 代表输入\n",
    "        print('>', pair[0])\n",
    "        # = 代表正确的输出\n",
    "        print('=', pair[1])\n",
    "        # 调用evaluate进行预测\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        # 将结果连成句子\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        # < 代表模型的输出\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m getting tired of this .\n",
      "= ca me fatigue .\n",
      "< je suis assez d d . <EOS>\n",
      "\n",
      "> he is tired of watching television .\n",
      "= il est fatigue de regarder la television .\n",
      "< il a a d de . . <EOS>\n",
      "\n",
      "> he s lived here his entire life .\n",
      "= il a vecu ici sa vie entiere .\n",
      "< il est de de son . <EOS>\n",
      "\n",
      "> i m chicken .\n",
      "= j ai les foies .\n",
      "< je suis desole . <EOS>\n",
      "\n",
      "> they re in danger .\n",
      "= elles sont en danger .\n",
      "< ils sont en . . <EOS>\n",
      "\n",
      "> you re the one who trained me .\n",
      "= vous etes celui qui m a formee .\n",
      "< c est vous qui m a . . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调用evaluateRandomly进行模型测试，将编码器对象encoder1，码器对象attn_decoder1传入其中\n",
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nous', 'sommes', 'tous', 'a', '.', '<EOS>']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \"we re both teachers .\"\n",
    "# 调用评估函数\n",
    "output_words, attentions = evaluate(\n",
    "encoder1, attn_decoder1, sentence)\n",
    "print(output_words)\n",
    "# 将attention张量转化成numpy, 使用matshow绘制\n",
    "plt.matshow(attentions.numpy())\n",
    "# 保存图像\n",
    "plt.savefig(\"./s2s_attn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention图像的纵坐标代表输入的源语言各个词汇对应的索引, 0-6分别对应[\"we\", \"re\", \"both\", \"teachers\", \".\", \"\"], 纵坐标代表生成的目标语言各个词汇对应的索引, 0-7代表['nous', 'sommes', 'toutes', 'deux', 'enseignantes', '.', ''], 图中浅色小方块(颜色越浅说明影响越大)代表词汇之间的影响关系, 比如源语言的第1个词汇对生成目标语言的第1个词汇影响最大, 源语言的第4，5个词对生成目标语言的第5个词会影响最大, 通过这样的可视化图像, 我们可以知道Attention的效果好坏, 与我们人为去判定到底还有多大的差距. 进而衡量我们训练模型的可用性.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f187313e0c52355639775e09fbea25117775c8ec6583301a0d8bdbe7dce22bd7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
