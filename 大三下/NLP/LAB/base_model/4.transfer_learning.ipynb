{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迁移学习\n",
    "## 1.1 预训练模型(Pretrained model)\n",
    "\n",
    "一般情况下预训练模型都是大型模型，具备复杂的网络结构，众多的参数量，以及在足够大的数据集下进行训练而产生的模型. 在NLP领域，预训练模型往往是语言模型，因为语言模型的训练是无监督的，可以获得大规模语料，同时语言模型又是许多典型NLP任务的基础，如机器翻译，文本生成，阅读理解等，常见的预训练模型有BERT, GPT, roBERTa, transformer-XL等.\n",
    "\n",
    "## 1.2 微调(Fine-tuning)\n",
    "\n",
    "根据给定的预训练模型，改变它的部分参数或者为其新增部分输出结构后，通过在小部分数据集上训练，来使整个模型更好的适应特定任务.\n",
    "\n",
    "## 1.3 两种迁移方式\n",
    "- 直接使用预训练模型，进行相同任务的处理，不需要调整参数或模型结构，这些模型开箱即用。但是这种情况一般只适用于普适任务, 如：fasttest工具包中预训练的词向量模型。另外，很多预训练模型开发者为了达到开箱即用的效果，将模型结构分各个部分保存为不同的预训练模型，提供对应的加载方法来完成特定目标.\n",
    "- 更加主流的迁移学习方式是发挥预训练模型特征抽象的能力，然后再通过微调的方式，通过训练更新小部分参数以此来适应不同的任务。这种迁移方式需要提供小部分的标注数据来进行监督学习.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP中的常用预训练模型\n",
    "- BERT\n",
    "- GPT\n",
    "- Transformer-XL\n",
    "- XLNet\n",
    "- XLM\n",
    "- RoBERTa\n",
    "- DistilBERT\n",
    "- ALBERT\n",
    "- T5\n",
    "\n",
    "## BERT及其变体\n",
    "- bert-base-uncased: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在小写的英文文本上进行训练而得到.\n",
    "- bert-large-uncased: 编码器具有24个隐层, 输出1024维张量, 16个自注意力头, 共340M参数量, 在小写的英文文本上进行训练而得到.\n",
    "- bert-base-cased: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在不区分大小写的英文文本上进行训练而得到.\n",
    "- bert-large-cased: 编码器具有24个隐层, 输出1024维张量, 16个自注意力头, 共340M参数量, 在不区分大小写的英文文本上进行训练而得到.\n",
    "- bert-base-multilingual-uncased: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在小写的102种语言文本上进行训练而得到.\n",
    "- bert-large-multilingual-uncased: 编码器具有24个隐层, 输出1024维张量, 16个自注意力头, 共340M参数量, 在小写的102种语言文本上进行训练而得到.\n",
    "- bert-base-chinese: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在简体和繁体中文文本上进行训练而得到.\n",
    "\n",
    "## BERT模型结构\n",
    "\n",
    "BERT是2018年10月由Google AI研究院提出的一种预训练模型.\n",
    "- BERT的全称是Bidirectional Encoder Representation from Transformers.\n",
    "- BERT在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类, 并且在11种不同NLP测试中创出SOTA表现. 包括将GLUE基准推高至80.4% (绝对改进7.6%), MultiNLI准确度达到86.7% (绝对改进5.6%). 成为NLP发展史上的里程碑式的模型成就.\n",
    "- BERT的两大预训练任务.\n",
    "    1. MLM任务(Masked Language Model), 在原始文本中随机抽取15%的token参与任务.\n",
    "        - 在80%概率下, 用[MASK]替换该token.\n",
    "        - 在10%概率下, 用一个随机的单词替换该token.\n",
    "        - 在10%概率下, 保持该token不变.\n",
    "    2. NSP任务(Next Sentence Prediction), 采用的方式是输入句子对(A, B), 模型预测句子B是不是句子A的真实的下一句话.\n",
    "        - 其中50%的B是原始文本中真实跟随A的下一句话.(标记为IsNext, 代表正样本)\n",
    "        - 其中50%的B是原始文本中随机抽取的一句话. (标记为NotNext, 代表负样本)\n",
    "\n",
    "<img src='bert.jpg' width=\"40%\" height=\"40%\">\n",
    "\n",
    "- 三层基础结构\n",
    "    - 最底层的Embedding模块, 包括Token Embeddings, Segment Embeddings, Position Embeddings.\n",
    "    - 中间层的Transformer模块, 只使用了经典Transformer架构中的Encoder部分.\n",
    "    - 最上层的预微调模块, 具体根据不同的任务类型来做相应的处理.\n",
    "    \n",
    "<img src='bert_finetune.png' width=\"40%\" height=\"40%\">\n",
    "\n",
    "- 两个输出模式\n",
    "    - sequence-level的分类任务, BERT直接取第一个[CLS] token 的final hidden state, 再加一层全连接层后进行softmax来预测最终的标签.\n",
    "    - token-level的分类任务,BERT可以获取每一个token的词向量输出，做分类\n",
    "\n",
    "## BERT等预训练语言模型的使用\n",
    "使用：https://huggingface.co/bert-base-chinese 平台，进行下载并使用\n",
    "\n",
    "<img src='huggingface平台的使用.jpg' width=\"40%\" height=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP中的标准数据集\n",
    "\n",
    "GLUE由纽约大学, 华盛顿大学, Google联合推出, 涵盖不同NLP任务类型, 截止至2020年1月其中包括11个子任务数据集, 成为衡量NLP研究发展的衡量标准.\n",
    "\n",
    "- CoLA 数据集:CoLA(The Corpus of Linguistic Acceptability，语言可接受性语料库)纽约大学发布的有关语法的数据集.本质: 是对一个给定句子，判定其是否语法正确的单个句子的**文本二分类任务**.\n",
    "- SST-2 数据集：SST-2(The Stanford Sentiment Treebank，斯坦福情感树库),单句子分类任务，包含电影评论中的句子和它们情感的人类注释.本质:句子级别的**二分类任务**\n",
    "- MRPC 数据集:MRPC(The Microsoft Research Paraphrase Corpus，微软研究院释义语料库),相似性和释义任务，是从在线新闻源中自动抽取句子对语料库，并人工注释句子对中的句子是否在语义上等效。本质:句子级别的**二分类任务**\n",
    "- STS-B 数据集:STSB(The Semantic Textual Similarity Benchmark，语义文本相似性基准测试).本质: 回归任务/句子对的文本五分类任务\n",
    "- QQP 数据集:数据集释义: QQP(The Quora Question Pairs, Quora问题对数集),相似性和释义任务，是社区问答网站Quora中问题对的集合。本质: 句子对的**二分类任务**\n",
    "- MNLI/SNLI 数据集:MNLI(The Multi-Genre Natural Language Inference Corpus, 多类型自然语言推理数据库).本质: 句子对的**三分类任务**\n",
    "- QNLI/RTE/WNLI 数据集:自然语言推断任务.本质: QNLI是二分类任务. RTE是二分类任务. WNLI是二分类任务."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f187313e0c52355639775e09fbea25117775c8ec6583301a0d8bdbe7dce22bd7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
