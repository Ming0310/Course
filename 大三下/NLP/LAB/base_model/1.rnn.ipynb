{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "\n",
    "RNN(Recurrent Neural Network), 中文称作循环神经网络, 它一般以序列数据为输入, 通过网络内部的结构设计有效捕捉序列之间的关系特征, 一般也是以序列形式进行输出.\n",
    "\n",
    "<img src='rnn.png'>\n",
    "\n",
    "\n",
    "<img src='pytorch_rnn.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照输入和输出的结构进行分类:\n",
    "\n",
    "- N vs N - RNN\n",
    "- N vs 1 - RNN\n",
    "- 1 vs N - RNN\n",
    "- N vs M - RNN\n",
    "\n",
    "按照RNN的内部构造进行分类:\n",
    "\n",
    "* 传统RNN\n",
    "* LSTM\n",
    "* Bi-LSTM\n",
    "* GRU\n",
    "* Bi-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 传统RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2322,  0.7129,  0.2137,  1.0382,  1.5742],\n",
      "         [-0.3869, -0.4696,  0.6525, -0.6911, -1.5156],\n",
      "         [ 0.1491, -0.7204,  0.4863, -1.8228,  1.3137]],\n",
      "\n",
      "        [[-1.4882,  0.4533, -0.1501,  0.3709,  0.1902],\n",
      "         [-0.1369, -0.3171, -1.9456, -0.0372, -1.3665],\n",
      "         [-1.0373, -0.3875, -0.9419,  0.7594, -1.6629]]])\n",
      "torch.Size([2, 3, 6])\n",
      "torch.Size([1, 3, 6])\n",
      "tensor([[[-1.5192e-01,  5.3846e-01,  7.9333e-02, -3.8676e-01,  6.6025e-01,\n",
      "           2.5935e-01],\n",
      "         [ 1.1805e-01, -7.5385e-01, -5.6800e-01, -6.3544e-01,  1.2743e-01,\n",
      "           2.2006e-01],\n",
      "         [ 8.4040e-01, -1.2047e-01,  5.5674e-01,  8.2013e-01,  6.2724e-01,\n",
      "           7.7619e-01]],\n",
      "\n",
      "        [[ 4.0441e-01,  1.2496e-03, -7.8893e-02, -5.1879e-02,  7.0065e-01,\n",
      "          -1.3898e-01],\n",
      "         [ 6.1530e-01,  3.0988e-01, -4.8052e-01,  3.5472e-01,  7.3634e-03,\n",
      "          -6.0797e-04],\n",
      "         [ 1.0240e-01, -2.7265e-01, -6.0495e-01, -9.7269e-02, -1.5060e-01,\n",
      "          -3.6580e-01]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 4.0441e-01,  1.2496e-03, -7.8893e-02, -5.1879e-02,  7.0065e-01,\n",
      "          -1.3898e-01],\n",
      "         [ 6.1530e-01,  3.0988e-01, -4.8052e-01,  3.5472e-01,  7.3634e-03,\n",
      "          -6.0797e-04],\n",
      "         [ 1.0240e-01, -2.7265e-01, -6.0495e-01, -9.7269e-02, -1.5060e-01,\n",
      "          -3.6580e-01]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 导入工具包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# (input_size,hidden_size,num_layers)\n",
    "rnn = nn.RNN(5, 6, 1)\n",
    "# (sequence_length,batch_size,input_size)\n",
    "input = torch.randn(2, 3, 5)\n",
    "print(input)\n",
    "# (num_layers*num_directions,batch_size,hidden_size)\n",
    "h0 = torch.randn(1, 3, 6)\n",
    "# output：（sequence_length,batch_size,num_directions*hidden_size）\n",
    "# hn：(num_layers*num_directions,batch_size,hidden_size)\n",
    "output, hn = rnn(input, h0)\n",
    "print(output.shape)\n",
    "print(hn.shape)\n",
    "print(output)\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 5])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(10, 5, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 5)\n",
    "output, hn = rnn(input, h0)\n",
    "print(output.shape)\n",
    "print(hn.shape)\n",
    "# print(output)\n",
    "# print(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='带batch的rnn.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统RNN的优势:\n",
    "- 由于内部结构简单, 对计算资源要求低, 相比之后我们要学习的RNN变体:LSTM和GRU模型参数总量少了很多, 在短序列任务上性能和效果都表现优异.\n",
    "\n",
    "传统RNN的缺点:\n",
    "- 传统RNN在解决长序列之间的关联时, 通过实践，证明经典RNN表现很差, 原因是在进行反向传播的时候, 过长的序列导致梯度的计算异常, 发生梯度消失或爆炸.\n",
    "\n",
    "<img src='rnn公式.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LSTM\n",
    "\n",
    "LSTM（Long Short-Term Memory）也称长短时记忆结构, 它是传统RNN的变体, 与经典RNN相比能够有效捕捉长序列之间的语义关联, 缓解梯度消失或爆炸现象. 同时LSTM的结构更复杂, 它的核心结构可以分为四个部分去解析:\n",
    "\n",
    "- 遗忘门\n",
    "- 输入门\n",
    "- 细胞状态\n",
    "- 输出门\n",
    "\n",
    "<img src='LSTM.png'>\n",
    "\n",
    "公式：\n",
    "<img src='LSTM公式.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 BiLSTM\n",
    "Bi-LSTM即双向LSTM, 它没有改变LSTM本身任何的内部结构, 只是将LSTM应用两次且方向不同, 再将两次得到的LSTM结果进行拼接作为最终输出.\n",
    "\n",
    "<img src='BiLSTM.jpg'>\n",
    "\n",
    "我们看到图中对\"我爱中国\"这句话或者叫这个输入序列, 进行了从左到右和从右到左两次LSTM处理, 将得到的结果张量进行了拼接作为最终输出. 这种结构能够捕捉语言语法中一些特定的前置或后置特征, 增强语义关联,但是模型参数和计算复杂度也随之增加了一倍, 一般需要对语料和计算资源进行评估后决定是否使用该结构."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 6])\n",
      "torch.Size([2, 3, 6])\n",
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.3073,  0.4991, -0.0824, -0.1701, -0.1884,  0.1686],\n",
      "         [ 0.1054, -0.2324,  0.3210, -0.0072, -0.5835,  0.0233],\n",
      "         [ 0.1306,  0.1611,  0.0150, -0.3076, -0.0164, -0.0344]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[-0.1743, -0.0666,  0.4232,  0.0571, -0.2901, -0.0446],\n",
      "         [ 0.3110,  0.1535,  0.4284, -0.2029, -0.1265,  0.1359],\n",
      "         [-0.0091,  0.2276, -0.2433, -0.1972, -0.1253,  0.0496]],\n",
      "\n",
      "        [[ 0.3073,  0.4991, -0.0824, -0.1701, -0.1884,  0.1686],\n",
      "         [ 0.1054, -0.2324,  0.3210, -0.0072, -0.5835,  0.0233],\n",
      "         [ 0.1306,  0.1611,  0.0150, -0.3076, -0.0164, -0.0344]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[-0.4282, -0.1159,  1.2232,  0.1876, -0.3474, -0.1521],\n",
      "         [ 0.4463,  0.2309,  0.9582, -2.0603, -0.2910,  0.7462],\n",
      "         [-0.0238,  0.8515, -0.5805, -0.6616, -0.1763,  0.1481]],\n",
      "\n",
      "        [[ 0.5873,  0.7667, -0.1498, -0.2554, -0.6461,  0.5758],\n",
      "         [ 0.2444, -0.4834,  0.7372, -0.0147, -1.3208,  0.0335],\n",
      "         [ 0.2598,  0.2508,  0.0592, -0.4334, -0.0381, -0.0904]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 定义LSTM的参数含义: (input_size, hidden_size, num_layers)\n",
    "# 定义输入张量的参数含义: (sequence_length, batch_size, input_size)\n",
    "# 定义隐藏层初始张量和细胞初始状态张量的参数含义:\n",
    "# (num_layers * num_directions, batch_size, hidden_size)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# rnn = nn.LSTM(5, 6, 2, bidirectional=True)\n",
    "# input = torch.randn(1, 3, 5)\n",
    "# h0 = torch.randn(4, 3, 6)\n",
    "# c0 = torch.randn(4, 3, 6)\n",
    "rnn = nn.LSTM(5, 6, 2)\n",
    "input = torch.randn(1, 3, 5)\n",
    "h0 = torch.randn(2, 3, 6)\n",
    "c0 = torch.randn(2, 3, 6)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))\n",
    "print(output.shape)\n",
    "print(hn.shape)\n",
    "print(cn.shape)\n",
    "print(output)\n",
    "print(hn)\n",
    "print(cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM优势:\n",
    "- LSTM的门结构能够有效减缓长序列问题中可能出现的梯度消失或爆炸, 虽然并不能杜绝这种现象, 但在更长的序列问题上表现优于传统RNN.\n",
    "\n",
    "LSTM缺点:\n",
    "- 由于内部结构相对较复杂, 因此训练效率在同等算力下较传统RNN低很多."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 GRU\n",
    "\n",
    "GRU（Gated Recurrent Unit）也称门控循环单元结构, 它也是传统RNN的变体, 同LSTM一样能够有效捕捉长序列之间的语义关联, 缓解梯度消失或爆炸现象. 同时它的结构和计算要比LSTM更简单, 它的核心结构可以分为两个部分去解析:\n",
    "\n",
    "- 更新门\n",
    "- 重置门\n",
    "\n",
    "<img src='gru.png'>\n",
    "\n",
    "公式in pytorch\n",
    "<img src='gru公式.jpg'>\n",
    "计算更新门和重置门的门值, 分别是z(t)和r(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 20])\n",
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.GRU(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)\n",
    "print(hn.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 BiGRU\n",
    "同上，设置bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU的优势:\n",
    "- GRU和LSTM作用相同, 在捕捉长序列语义关联时, 能有效抑制梯度消失或爆炸, 效果都优于传统RNN且计算复杂度相比LSTM要小.\n",
    "\n",
    "GRU的缺点:\n",
    "- GRU仍然不能完全解决梯度消失问题, 同时其作用RNN的变体, 有着RNN结构本身的一大弊端, 即不可并行计算, 这在数据量和模型体量逐步增大的未来, 是RNN发展的关键瓶颈.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f187313e0c52355639775e09fbea25117775c8ec6583301a0d8bdbe7dce22bd7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
